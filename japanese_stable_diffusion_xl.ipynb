{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxKn0STrLO3F"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Stability-AI/model-demo-notebooks/blob/main/japanese_stable_diffusion_xl.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "\n",
        "# Japanese Stable Diffusion XL Demo\n",
        "This is a demo for [Japanese Stable Diffusion XL](https://huggingface.co/stabilityai/japanese-stable-diffusion-xl) from [Stability AI](https://stability.ai/).\n",
        "\n",
        "- Blog: https://ja.stability.ai/blog/japanese-stable-diffusion-xl\n",
        "- Twitter: https://twitter.com/StabilityAI_JP\n",
        "- Discord: https://discord.com/invite/StableJP\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "K92bGOlkLNC5"
      },
      "outputs": [],
      "source": [
        "#@title Setup\n",
        "!nvidia-smi\n",
        "!pip install 'diffusers>=0.23.0' transformers sentencepiece gradio accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "uNNRyH3WLw84"
      },
      "outputs": [],
      "source": [
        "# @title Login HuggingFace\n",
        "!huggingface-cli login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "mKC-h58pLyOu"
      },
      "outputs": [],
      "source": [
        "#@title Load JSDXL\n",
        "\n",
        "# copied from https://huggingface.co/stabilityai/japanese-stable-diffusion-xl/blob/main/modeling_clipnull.py\n",
        "from dataclasses import dataclass\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "from diffusers.configuration_utils import register_to_config, ConfigMixin\n",
        "from diffusers.models.modeling_utils import ModelMixin\n",
        "from diffusers.utils import BaseOutput\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class CLIPNullTextOutput(BaseOutput):\n",
        "    text_embeds: torch.FloatTensor\n",
        "    last_hidden_state: torch.FloatTensor\n",
        "\n",
        "\n",
        "class CLIPNullTextModel(ModelMixin, ConfigMixin):\n",
        "    @register_to_config\n",
        "    def __init__(\n",
        "        self,\n",
        "        hidden_size: int = 1280,\n",
        "        always_return_pooled: bool = True,\n",
        "        max_length: int = 77,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.return_pooled = always_return_pooled\n",
        "        self.max_length = max_length\n",
        "        self.register_parameter(\n",
        "            \"z\", nn.Parameter(torch.zeros((1, self.max_length, self.hidden_size)))\n",
        "        )\n",
        "        self.register_parameter(\n",
        "            \"pooled\",\n",
        "            nn.Parameter(\n",
        "                torch.zeros((1, self.hidden_size)) if always_return_pooled else None\n",
        "            ),\n",
        "        )\n",
        "\n",
        "    def forward(self, bsz: int = 1, return_dict: bool = True):\n",
        "        z = self.z.expand(bsz, -1, -1)\n",
        "        pooled = None\n",
        "        if self.return_pooled:\n",
        "            pooled = self.pooled.expand(bsz, -1)\n",
        "\n",
        "        if not return_dict:\n",
        "            return (pooled, z)\n",
        "\n",
        "        return CLIPNullTextOutput(\n",
        "            text_embeds=pooled,\n",
        "            last_hidden_state=z,\n",
        "        )\n",
        "\n",
        "# img2img pipeline\n",
        "from typing import Optional\n",
        "\n",
        "import torch\n",
        "from diffusers import StableDiffusionXLImg2ImgPipeline\n",
        "from diffusers.image_processor import VaeImageProcessor\n",
        "from diffusers.loaders import (\n",
        "    StableDiffusionXLLoraLoaderMixin,\n",
        "    TextualInversionLoaderMixin,\n",
        ")\n",
        "from diffusers.models import AutoencoderKL, UNet2DConditionModel\n",
        "from diffusers.models.lora import adjust_lora_scale_text_encoder\n",
        "from diffusers.schedulers import KarrasDiffusionSchedulers\n",
        "from diffusers.utils import (\n",
        "    USE_PEFT_BACKEND,\n",
        "    is_invisible_watermark_available,\n",
        "    logging,\n",
        "    scale_lora_layers,\n",
        "    unscale_lora_layers,\n",
        ")\n",
        "from transformers import AutoTokenizer, CLIPTextModelWithProjection\n",
        "# from .modeling_clipnull import CLIPNullTextModel\n",
        "\n",
        "if is_invisible_watermark_available():\n",
        "    from diffusers.pipelines.stable_diffusion_xl.watermark import (\n",
        "        StableDiffusionXLWatermarker,\n",
        "    )\n",
        "\n",
        "\n",
        "class JapaneseStableDiffusionXLImg2ImgPipeline(StableDiffusionXLImg2ImgPipeline):\n",
        "    model_cpu_offload_seq = \"text_encoder->null_encoder->unet->vae\"\n",
        "    _optional_components = [\"tokenizer\", \"text_encoder\", \"null_encoder\"]\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        vae: AutoencoderKL,\n",
        "        text_encoder: CLIPTextModelWithProjection,\n",
        "        tokenizer: AutoTokenizer,\n",
        "        unet: UNet2DConditionModel,\n",
        "        scheduler: KarrasDiffusionSchedulers,\n",
        "        null_encoder: Optional[CLIPNullTextModel] = None,\n",
        "        requires_aesthetics_score: bool = False,\n",
        "        force_zeros_for_empty_prompt: bool = True,\n",
        "        add_watermarker: Optional[bool] = None,\n",
        "    ):\n",
        "        if null_encoder is None:\n",
        "            null_encoder = CLIPNullTextModel(\n",
        "                hidden_size=1280,\n",
        "                max_length=tokenizer.model_max_length,\n",
        "                always_return_pooled=True,\n",
        "            )\n",
        "        self.register_modules(\n",
        "            vae=vae,\n",
        "            text_encoder=text_encoder,\n",
        "            tokenizer=tokenizer,\n",
        "            unet=unet,\n",
        "            scheduler=scheduler,\n",
        "            null_encoder=null_encoder,\n",
        "        )\n",
        "        self.register_to_config(force_zeros_for_empty_prompt=force_zeros_for_empty_prompt)\n",
        "        self.register_to_config(requires_aesthetics_score=requires_aesthetics_score)\n",
        "        self.vae_scale_factor = 2 ** (len(self.vae.config.block_out_channels) - 1)\n",
        "        self.image_processor = VaeImageProcessor(vae_scale_factor=self.vae_scale_factor)\n",
        "        self.default_sample_size = self.unet.config.sample_size\n",
        "\n",
        "        add_watermarker = (\n",
        "            add_watermarker\n",
        "            if add_watermarker is not None\n",
        "            else is_invisible_watermark_available()\n",
        "        )\n",
        "\n",
        "        if add_watermarker:\n",
        "            self.watermark = StableDiffusionXLWatermarker()\n",
        "        else:\n",
        "            self.watermark = None\n",
        "        self.text_encoder_2 = None\n",
        "\n",
        "    def encode_prompt(\n",
        "        self,\n",
        "        prompt: str,\n",
        "        device: Optional[torch.device] = None,\n",
        "        num_images_per_prompt: int = 1,\n",
        "        do_classifier_free_guidance: bool = True,\n",
        "        negative_prompt: Optional[str] = None,\n",
        "        prompt_embeds: Optional[torch.FloatTensor] = None,\n",
        "        negative_prompt_embeds: Optional[torch.FloatTensor] = None,\n",
        "        pooled_prompt_embeds: Optional[torch.FloatTensor] = None,\n",
        "        negative_pooled_prompt_embeds: Optional[torch.FloatTensor] = None,\n",
        "        lora_scale: Optional[float] = None,\n",
        "        clip_skip: Optional[int] = None,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        device = device or self._execution_device\n",
        "\n",
        "        # set lora scale so that monkey patched LoRA\n",
        "        # function of text encoder can correctly access it\n",
        "        if lora_scale is not None and isinstance(\n",
        "            self, StableDiffusionXLLoraLoaderMixin\n",
        "        ):\n",
        "            self._lora_scale = lora_scale\n",
        "\n",
        "            # dynamically adjust the LoRA scale\n",
        "            if self.text_encoder is not None:\n",
        "                if not USE_PEFT_BACKEND:\n",
        "                    adjust_lora_scale_text_encoder(self.text_encoder, lora_scale)\n",
        "                else:\n",
        "                    scale_lora_layers(self.text_encoder, lora_scale)\n",
        "\n",
        "        prompt = [prompt] if isinstance(prompt, str) else prompt\n",
        "\n",
        "        if prompt is not None:\n",
        "            batch_size = len(prompt)\n",
        "        else:\n",
        "            batch_size = prompt_embeds.shape[0]\n",
        "\n",
        "        if prompt_embeds is None:\n",
        "            # textual inversion: procecss multi-vector tokens if necessary\n",
        "            prompt_embeds_list = []\n",
        "            if isinstance(self, TextualInversionLoaderMixin):\n",
        "                prompt = self.maybe_convert_prompt(prompt, self.tokenizer)\n",
        "\n",
        "            text_inputs = self.tokenizer(\n",
        "                prompt,\n",
        "                padding=\"max_length\",\n",
        "                max_length=self.tokenizer.model_max_length,\n",
        "                truncation=True,\n",
        "                return_tensors=\"pt\",\n",
        "            )\n",
        "\n",
        "            text_input_ids = text_inputs.input_ids\n",
        "            untruncated_ids = self.tokenizer(\n",
        "                prompt, padding=\"longest\", return_tensors=\"pt\"\n",
        "            ).input_ids\n",
        "\n",
        "            if untruncated_ids.shape[-1] >= text_input_ids.shape[\n",
        "                -1\n",
        "            ] and not torch.equal(text_input_ids, untruncated_ids):\n",
        "                removed_text = self.tokenizer.batch_decode(\n",
        "                    untruncated_ids[:, self.tokenizer.model_max_length - 1 : -1]\n",
        "                )\n",
        "                logger.warning(\n",
        "                    \"The following part of your input was truncated because CLIP can only handle sequences up to\"\n",
        "                    f\" {self.tokenizer.model_max_length} tokens: {removed_text}\"\n",
        "                )\n",
        "\n",
        "            prompt_embeds = self.text_encoder(\n",
        "                text_input_ids.to(device), output_hidden_states=True\n",
        "            )\n",
        "            if clip_skip is None:\n",
        "                prompt_embeds = prompt_embeds.hidden_states[-2]\n",
        "            else:\n",
        "                # \"2\" because SDXL always indexes from the penultimate layer.\n",
        "                prompt_embeds = prompt_embeds.hidden_states[-(clip_skip + 2)]\n",
        "            prompt_embeds_list.append(prompt_embeds)\n",
        "            bsz = prompt_embeds.size(0)\n",
        "\n",
        "            pooled_prompt_embeds, prompt_embeds_2 = self.null_encoder(\n",
        "                bsz, return_dict=False\n",
        "            )\n",
        "            prompt_embeds_2 = prompt_embeds_2.to(\n",
        "                prompt_embeds.device, prompt_embeds_2.dtype\n",
        "            )\n",
        "            pooled_prompt_embeds = pooled_prompt_embeds.to(\n",
        "                prompt_embeds.device, prompt_embeds.dtype\n",
        "            )\n",
        "            prompt_embeds_list.append(prompt_embeds_2)\n",
        "            prompt_embeds = torch.concat(prompt_embeds_list, dim=-1)\n",
        "\n",
        "        # get unconditional embeddings for classifier free guidance\n",
        "        zero_out_negative_prompt = (\n",
        "            negative_prompt is None and self.config.force_zeros_for_empty_prompt\n",
        "        )\n",
        "        if (\n",
        "            do_classifier_free_guidance\n",
        "            and negative_prompt_embeds is None\n",
        "            and zero_out_negative_prompt\n",
        "        ):\n",
        "            negative_prompt_embeds = torch.zeros_like(prompt_embeds)\n",
        "            negative_pooled_prompt_embeds = torch.zeros_like(pooled_prompt_embeds)\n",
        "        elif do_classifier_free_guidance and negative_prompt_embeds is None:\n",
        "            negative_prompt = negative_prompt or \"\"\n",
        "\n",
        "            # normalize str to list\n",
        "            negative_prompt = (\n",
        "                batch_size * [negative_prompt]\n",
        "                if isinstance(negative_prompt, str)\n",
        "                else negative_prompt\n",
        "            )\n",
        "\n",
        "            if prompt is not None and type(prompt) is not type(negative_prompt):\n",
        "                raise TypeError(\n",
        "                    f\"`negative_prompt` should be the same type to `prompt`, but got {type(negative_prompt)} !=\"\n",
        "                    f\" {type(prompt)}.\"\n",
        "                )\n",
        "            elif batch_size != len(negative_prompt):\n",
        "                raise ValueError(\n",
        "                    f\"`negative_prompt`: {negative_prompt} has batch size {len(negative_prompt)}, but `prompt`:\"\n",
        "                    f\" {prompt} has batch size {batch_size}. Please make sure that passed `negative_prompt` matches\"\n",
        "                    \" the batch size of `prompt`.\"\n",
        "                )\n",
        "\n",
        "            negative_prompt_embeds_list = []\n",
        "            if isinstance(self, TextualInversionLoaderMixin):\n",
        "                negative_prompt = self.maybe_convert_prompt(\n",
        "                    negative_prompt, self.tokenizer\n",
        "                )\n",
        "\n",
        "            max_length = prompt_embeds.shape[1]\n",
        "            uncond_input = self.tokenizer(\n",
        "                negative_prompt,\n",
        "                padding=\"max_length\",\n",
        "                max_length=max_length,\n",
        "                truncation=True,\n",
        "                return_tensors=\"pt\",\n",
        "            )\n",
        "\n",
        "            negative_prompt_embeds = self.text_encoder(\n",
        "                uncond_input.input_ids.to(device),\n",
        "                output_hidden_states=True,\n",
        "            )\n",
        "            negative_prompt_embeds = negative_prompt_embeds.hidden_states[-2]\n",
        "            negative_prompt_embeds_list.append(negative_prompt_embeds)\n",
        "\n",
        "            bsz = negative_prompt_embeds.size(0)\n",
        "            negative_prompt_embeds_2 = torch.zeros(\n",
        "                (bsz, negative_prompt_embeds.size(1), self.null_encoder.hidden_size),\n",
        "                device=negative_prompt_embeds.device,\n",
        "            )\n",
        "            negative_pooled_prompt_embeds = torch.zeros(\n",
        "                (bsz, self.null_encoder.hidden_size), device=self.device\n",
        "            )\n",
        "            negative_prompt_embeds_list.append(negative_prompt_embeds_2)\n",
        "\n",
        "            negative_prompt_embeds = torch.concat(negative_prompt_embeds_list, dim=-1)\n",
        "\n",
        "        prompt_embeds = prompt_embeds.to(dtype=self.unet.dtype, device=device)\n",
        "\n",
        "        bs_embed, seq_len, _ = prompt_embeds.shape\n",
        "        # duplicate text embeddings for each generation per prompt, using mps friendly method\n",
        "        prompt_embeds = prompt_embeds.repeat(1, num_images_per_prompt, 1)\n",
        "        prompt_embeds = prompt_embeds.view(\n",
        "            bs_embed * num_images_per_prompt, seq_len, -1\n",
        "        )\n",
        "\n",
        "        if do_classifier_free_guidance:\n",
        "            # duplicate unconditional embeddings for each generation per prompt, using mps friendly method\n",
        "            seq_len = negative_prompt_embeds.shape[1]\n",
        "\n",
        "            negative_prompt_embeds = negative_prompt_embeds.to(\n",
        "                dtype=self.unet.dtype, device=device\n",
        "            )\n",
        "\n",
        "            negative_prompt_embeds = negative_prompt_embeds.repeat(\n",
        "                1, num_images_per_prompt, 1\n",
        "            )\n",
        "            negative_prompt_embeds = negative_prompt_embeds.view(\n",
        "                batch_size * num_images_per_prompt, seq_len, -1\n",
        "            )\n",
        "\n",
        "        pooled_prompt_embeds = pooled_prompt_embeds.repeat(\n",
        "            1, num_images_per_prompt\n",
        "        ).view(bs_embed * num_images_per_prompt, -1)\n",
        "        if do_classifier_free_guidance:\n",
        "            negative_pooled_prompt_embeds = negative_pooled_prompt_embeds.repeat(\n",
        "                1, num_images_per_prompt\n",
        "            ).view(bs_embed * num_images_per_prompt, -1)\n",
        "\n",
        "        if self.text_encoder is not None:\n",
        "            if isinstance(self, StableDiffusionXLLoraLoaderMixin) and USE_PEFT_BACKEND:\n",
        "                # Retrieve the original scale by scaling back the LoRA layers\n",
        "                unscale_lora_layers(self.text_encoder, lora_scale)\n",
        "        return (\n",
        "            prompt_embeds,\n",
        "            negative_prompt_embeds,\n",
        "            pooled_prompt_embeds,\n",
        "            negative_pooled_prompt_embeds,\n",
        "        )\n",
        "\n",
        "    def check_inputs(\n",
        "        self,\n",
        "        prompt,\n",
        "        prompt_2,\n",
        "        strength,\n",
        "        num_inference_steps,\n",
        "        callback_steps,\n",
        "        negative_prompt=None,\n",
        "        negative_prompt_2=None,\n",
        "        prompt_embeds=None,\n",
        "        negative_prompt_embeds=None,\n",
        "        callback_on_step_end_tensor_inputs=None,\n",
        "    ):\n",
        "        assert (\n",
        "            prompt_2 is None\n",
        "        ), \"Japanese Stable Diffusion XL doesn't support `prompt_2` because there's only one text encoder.\"\n",
        "        assert (\n",
        "            negative_prompt_2 is None\n",
        "        ), \"Japanese Stable Diffusion XL doesn't support `prompt_2` because there's only one text encoder.\"\n",
        "        return super().check_inputs(\n",
        "            prompt,\n",
        "            None,\n",
        "            strength,\n",
        "            num_inference_steps,\n",
        "            callback_steps,\n",
        "            negative_prompt,\n",
        "            None,\n",
        "            prompt_embeds,\n",
        "            negative_prompt_embeds,\n",
        "            callback_on_step_end_tensor_inputs,\n",
        "        )\n",
        "\n",
        "\n",
        "# start loading pipeline\n",
        "from diffusers import DiffusionPipeline\n",
        "\n",
        "\n",
        "pipeline_type = \"txt2img\" # @param [\"txt2img\", \"img2img\"]\n",
        "pipeline_id = \"stabilityai/japanese-stable-diffusion-xl\"\n",
        "\n",
        "pipe = DiffusionPipeline.from_pretrained(\n",
        "    \"stabilityai/japanese-stable-diffusion-xl\",\n",
        "    trust_remote_code=True,\n",
        "    torch_dtype=torch.float16\n",
        ")\n",
        "\n",
        "if pipeline_type == \"img2img\":\n",
        "  pipe = JapaneseStableDiffusionXLImg2ImgPipeline(\n",
        "      vae=pipe.vae,\n",
        "      text_encoder=pipe.text_encoder,\n",
        "      tokenizer=pipe.tokenizer,\n",
        "      unet=pipe.unet,\n",
        "      scheduler=pipe.scheduler,\n",
        "      null_encoder=pipe.null_encoder\n",
        "  )\n",
        "\n",
        "# if using torch < 2.0\n",
        "# pipeline.enable_xformers_memory_efficient_attention()\n",
        "pipe.to(\"cuda\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "MPX7dCw1MAof"
      },
      "outputs": [],
      "source": [
        "# @title Launch the demo\n",
        "import random\n",
        "import gc\n",
        "import gradio as gr\n",
        "from diffusers.utils import make_image_grid\n",
        "\n",
        "\n",
        "def infer_func(\n",
        "    prompt,\n",
        "    scale=7.5,\n",
        "    steps=40,\n",
        "    W=1024,\n",
        "    H=1024,\n",
        "    n_samples=1,\n",
        "    seed=\"random\",\n",
        "    negative_prompt=\"\",\n",
        "    image=None,\n",
        "    strength=0.7,\n",
        "):\n",
        "    scale = float(scale)\n",
        "    steps = int(steps)\n",
        "    W = int(W)\n",
        "    H = int(H)\n",
        "    n_samples = int(n_samples)\n",
        "    if seed == \"random\":\n",
        "        seed = random.randint(0, 2**32)\n",
        "    seed = int(seed)\n",
        "    kwargs = {}\n",
        "    if pipeline_type == \"img2img\":\n",
        "        kwargs[\"image\"] = image\n",
        "        kwargs[\"strength\"] = strength\n",
        "    else:\n",
        "        kwargs[\"height\"] = H\n",
        "        kwargs[\"width\"] = W\n",
        "    images = pipe(\n",
        "        prompt=prompt,\n",
        "        negative_prompt=negative_prompt if len(negative_prompt) > 0 else None,\n",
        "        guidance_scale=scale,\n",
        "        generator=torch.Generator(device=\"cuda\").manual_seed(seed),\n",
        "        num_images_per_prompt=n_samples,\n",
        "        num_inference_steps=steps,\n",
        "        **kwargs\n",
        "    ).images\n",
        "    grid = make_image_grid(images, 1, len(images))\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    return grid, images, {\"seed\": seed}\n",
        "\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"# Japanese Stable Diffusion XL Demo\")\n",
        "    gr.Markdown(\n",
        "        \"\"\"[Japanese Stable Diffusion XL](https://huggingface.co/stabilityai/japanese-stable-diffusion-xl) is a Japanese-specific SDXL by [Stability AI](https://ja.stability.ai/).\n",
        "                - Blog: https://ja.stability.ai/blog/japanese-stable-diffusion-xl\n",
        "                - Twitter: https://twitter.com/StabilityAI_JP\n",
        "                - Discord: https://discord.com/invite/StableJP\"\"\"\n",
        "    )\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            prompt = gr.Textbox(label=\"prompt\", max_lines=1, value=\"カラフルなペンギン、アート\")\n",
        "            image = gr.Image(label=\"input_image\", visible=pipeline_type==\"img2img\", value=None, type=\"pil\")\n",
        "            scale = gr.Number(value=7.5, label=\"cfg_scale\")\n",
        "            strength = gr.Number(value=0.5, label=\"strength\")\n",
        "            steps = gr.Number(value=40, label=\"steps\")\n",
        "            width = gr.Number(value=1024, label=\"width\", visible=pipeline_type!=\"img2img\")\n",
        "            height = gr.Number(value=1024, label=\"height\", visible=pipeline_type!=\"img2img\")\n",
        "            n_samples = gr.Number(value=1, label=\"n_samples\", precision=0, maximum=5)\n",
        "            seed = gr.Text(value=\"42\", label=\"seed (integer or 'random')\")\n",
        "            negative_prompt = gr.Textbox(label=\"negative prompt\", value=\"\")\n",
        "            btn = gr.Button(\"Run\")\n",
        "        with gr.Column():\n",
        "            out = gr.Image(label=\"grid\")\n",
        "            gallery = gr.Gallery(label=\"Generated images\", show_label=False)\n",
        "            info = gr.JSON(label=\"sampling_info\")\n",
        "    inputs = [\n",
        "        prompt,\n",
        "        scale,\n",
        "        steps,\n",
        "        width,\n",
        "        height,\n",
        "        n_samples,\n",
        "        seed,\n",
        "        negative_prompt,\n",
        "        image,\n",
        "        strength,\n",
        "    ]\n",
        "    prompt.submit(infer_func, inputs=inputs, outputs=[out, gallery, info])\n",
        "    btn.click(infer_func, inputs=inputs, outputs=[out, gallery, info])\n",
        "\n",
        "demo.launch(debug=True, share=True, show_error=True)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}